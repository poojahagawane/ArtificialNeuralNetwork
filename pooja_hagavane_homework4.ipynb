{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The neural newtwork is now trained!!\n",
      "\n",
      "Please make a function call to testing_data function as follows:\n",
      "\n",
      "NN.testing_data(TEST_IMAGES, LABELS)\n"
     ]
    }
   ],
   "source": [
    "import cloudpickle as pickle\n",
    "mnist23 = pickle.load( open( \"mnist23.data\", \"rb\" ) )\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import decomposition\n",
    "pca = decomposition.PCA(n_components=392, whiten=True)\n",
    "pca.fit(mnist23.data)\n",
    "new_pca_data = pca.transform(mnist23.data) # transform input data of (12111,784) to (12111,392)\n",
    "\n",
    "X = new_pca_data.data #input matrix X\n",
    "data_size = len(X)\n",
    "y = []\n",
    "for i in range(len(mnist23.target)):\n",
    "    if(mnist23.target[i]==2):\n",
    "        y = np.append(y, [0])\n",
    "    else:\n",
    "        y = np.append(y, [1])\n",
    "y = y.reshape(-1,1) #Expected output y\n",
    "\n",
    "\n",
    "class Neural_Network(object):\n",
    "    def __init__(self): #parameters initialization\n",
    "        self.inputSize = 392\n",
    "        self.outputSize = 1\n",
    "        self.hiddenSize = 100        \n",
    "        self.W1 = []\n",
    "        self.W2 = []\n",
    "        for i in range(self.inputSize*self.hiddenSize):\n",
    "            self.W = np.random.uniform(-0.5, 0.5)\n",
    "            self.W1 = np.append(self.W1,self.W)        \n",
    "        self.W1 = np.reshape(self.W1,(self.inputSize,self.hiddenSize)) #initialization of weight matrix for layer 1 with random values from -0.5 to 0.5\n",
    "        for i in range(self.hiddenSize*self.outputSize):\n",
    "            self.W = np.random.uniform(-0.5, 0.5)\n",
    "            self.W2 = np.append(self.W2,self.W)      \n",
    "        self.W2 = np.reshape(self.W2,(self.hiddenSize, self.outputSize)) #initialization of weight matrix for layer 2 with random values from -0.5 to 0.5\n",
    "        self.b = (data_size,self.hiddenSize)\n",
    "        self.b1 = np.ones(self.b)  #intialization of bias for layer 1 with all ones\n",
    "        self.b = (data_size,1)\n",
    "        self.b2 = np.ones(self.b) #intialization of bias for layer 2 with all ones\n",
    "    \n",
    "    def sigmoid(self, var): # activation function  \n",
    "        return 1/(1+np.exp(-var))\n",
    "\n",
    "    def delta_sigmoid(self, var): #derivative of sigmoid\n",
    "        return var * (1 - var)     \n",
    "    \n",
    "    def forward(self, X): #forward propagation through the network\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1 # dot product of X (input) and first set of weights\n",
    "        self.a1 = self.sigmoid(self.z1) # activation function\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2 # dot product of hidden layer and second set of weights\n",
    "        a2 = self.sigmoid(self.z2) # final activation function\n",
    "        return a2 \n",
    "\n",
    "    def backward(self, X, y, a2): # backward propgate through the network\n",
    "        alpha = 0.001\n",
    "        self.a2_error = a2 - y # error in output\n",
    "        self.a2_delta = self.a2_error*self.delta_sigmoid(a2) # applying derivative of sigmoid to error        \n",
    "        self.a1_error = np.dot(self.a2_delta,np.transpose(self.W2)) # how much our hidden layer weights contributed to output error\n",
    "        self.a1_delta = self.a1_error*self.delta_sigmoid(self.a1) # applying derivative of sigmoid to a1 error\n",
    "        self.W1 = self.W1 - alpha * np.dot(np.transpose(X),self.a1_delta) # adjusting first set of weights\n",
    "        self.b1 = self.b1 - alpha * self.a1_delta # adjusting bias for layer 1\n",
    "        self.W2 = self.W2 - alpha * np.dot(np.transpose(self.a1),self.a2_delta) # adjusting second set of weights\n",
    "        self.b2 = self.b2 - alpha * self.a2_delta # adjusting bias for layer 2\n",
    "\n",
    "    def train (self, X, y): #trains nueral network \n",
    "        a2 = self.forward(X) # function call to forword propagation function\n",
    "        self.backward(X, y, a2) # function call to backword propagation function\n",
    "    \n",
    "    def predict (self, test_data, Final_W1, Final_W2): # predicts output for test data\n",
    "        output = []\n",
    "        self.z1 = np.dot(test_data, Final_W1) # dot product of X (input) and first set of weights\n",
    "        self.a1 = self.sigmoid(self.z1) # activation function\n",
    "        self.z2 = np.dot(self.a1, Final_W2) # dot product of hidden layer and second set of weights\n",
    "        ot = self.sigmoid(self.z2) # final activation function\n",
    "        for i in range(len(ot)):\n",
    "            if(ot[i]<0.5):\n",
    "                output = np.append(output,[2.])\n",
    "            else:\n",
    "                output = np.append(output,[3.])\n",
    "        return output\n",
    "    \n",
    "    def get_accuracy_test_data (self, expected_output, actual_output): # to get accuracy of test data\n",
    "        count = 0\n",
    "        for i in range(len(expected_output)):\n",
    "            if(actual_output[i]==expected_output[i]): # checks output equality\n",
    "                count = count + 1\n",
    "        accuracy = (count/len(expected_output))*100 # calculates percentage accuracy\n",
    "        print (\"Percentage accuracy for test data: \" + str(accuracy))\n",
    "        \n",
    "    def testing_data (self, test_data, labels):\n",
    "        from sklearn import decomposition\n",
    "        pca = decomposition.PCA(n_components=392, whiten=True)\n",
    "        pca.fit(test_data.data)\n",
    "        new_test_data = pca.transform(test_data.data) # transform test data of (x,784) to (x,392)\n",
    "        test_target = labels # Labels of test data\n",
    "        output = NN.predict(new_test_data, self.W1, self.W2) # predicted output\n",
    "        NN.get_accuracy_test_data(test_target,output) # testing accuracy of prediced output over expected output\n",
    "        \n",
    "NN = Neural_Network()\n",
    "for i in range(300): # trains the NN 300 times\n",
    "    NN.train(X, y)\n",
    "print (\"The neural newtwork is now trained!!\\n\\nPlease make a function call to testing_data function as follows:\\n\")\n",
    "print (\"NN.testing_data(TEST_IMAGES, LABELS)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage accuracy for test data: 91.48333333333333\n"
     ]
    }
   ],
   "source": [
    "NN.testing_data(mnist23.data[3000:9000], mnist23.target[3000:9000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
